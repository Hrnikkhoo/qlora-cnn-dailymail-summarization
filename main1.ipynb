{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#========================== Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes rouge-score torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda7c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù†Ø¯\n"
     ]
    }
   ],
   "source": [
    "#========================== ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù†Ø¯\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314a80cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0baaafe8ff41a0bd35078bf9d1a693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e58ea2736f5402882e6e78cc9a0d470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cea048dd1b41588414cac9a2953953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cd28b24f1e41c699f597bf7bc6026c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b322e87b1ab140dd8e7645b7343df657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ff857d541e4937bdd4d07b50f1b69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c20394c2bd44639428cf0647b5badc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!\n",
      "ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„: 1,100,048,384\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "#========================== Ù†Ù…Ø§ÛŒØ´ Ù…Ø³ÛŒØ± cache Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
    "import os\n",
    "from huggingface_hub import cached_assets_path\n",
    "\n",
    "# Ù…Ø³ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ cache Ø¯Ø± ÙˆÛŒÙ†Ø¯ÙˆØ²\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "print(f\" Ù…Ø³ÛŒØ± cache Ù…Ø¯Ù„â€ŒÙ‡Ø§: {cache_dir}\")\n",
    "print(f\" Ù…Ø¯Ù„ Ø§Ø² Hugging Face Hub Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯: https://huggingface.co/{MODEL_NAME}\")\n",
    "\n",
    "#========================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª 4-bit quantization Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù RAM Ùˆ GPU\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\" Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ {MODEL_NAME}...\")\n",
    "print(\" Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø± Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯ (Ø¯Ø§Ù†Ù„ÙˆØ¯ ~2.2GB)\")\n",
    "\n",
    "#========================== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#============================= Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "#=========================== Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ QLoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\" Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!\")\n",
    "print(f\" ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„: {model.num_parameters():,}\")\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ù…Ø³ÛŒØ± Ø¯Ù‚ÛŒÙ‚ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„\n",
    "model_path = os.path.join(cache_dir, f\"models--{MODEL_NAME.replace('/', '--')}\")\n",
    "if os.path.exists(model_path):\n",
    "    print(f\" Ù…Ø¯Ù„ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ea6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)\n",
    "# Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø³Ø±ÛŒØ¹â€ŒØªØ± ØªØ³Øª Ú©Ù†ÛŒØ¯ØŒ Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±Ø§ Ú©Ø§Ù‡Ø´ Ø¯Ù‡ÛŒØ¯\n",
    "\n",
    "# Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³Ø±ÛŒØ¹ (Ø­Ø¯ÙˆØ¯ 1-2 Ø³Ø§Ø¹Øª Ø±ÙˆÛŒ CPU):\n",
    "# MAX_TRAIN_SAMPLES = 1000\n",
    "# MAX_VAL_SAMPLES = 100\n",
    "# NUM_EPOCHS = 1\n",
    "\n",
    "# Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ú©Ø§Ù…Ù„ (9-15 Ø³Ø§Ø¹Øª Ø±ÙˆÛŒ CPU):\n",
    "MAX_TRAIN_SAMPLES = 5000\n",
    "MAX_VAL_SAMPLES = 500\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "print(\"âš ï¸ ØªÙˆØ¬Ù‡: Ø¨Ø¯ÙˆÙ† GPUØŒ Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ CPU Ø®ÛŒÙ„ÛŒ Ú©Ù†Ø¯ Ø§Ø³Øª (9-15 Ø³Ø§Ø¹Øª)\")\n",
    "print(\"ğŸ’¡ ØªÙˆØµÛŒÙ‡: Ø§Ø² Google Colab Ø¨Ø§ GPU Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯\")\n",
    "print(f\"ğŸ“Š ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ¹Ù„ÛŒ: {MAX_TRAIN_SAMPLES} Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ {NUM_EPOCHS} epoch\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6f939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª \n",
      " Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØªØ§Ø³Øª:\n",
      "  - Train: 287113 Ù†Ù…ÙˆÙ†Ù‡\n",
      "  - Validation: 13368 Ù†Ù…ÙˆÙ†Ù‡\n",
      "  - Test: 11490 Ù†Ù…ÙˆÙ†Ù‡\n",
      "\n",
      " Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª:\n",
      "Article (Ø§ÙˆÙ„ÛŒÙ† 200 Ú©Ø§Ø±Ø§Ú©ØªØ±): LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported Â£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on ...\n",
      "Highlights: Harry Potter star Daniel Radcliffe gets Â£20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n",
      " Ø¯ÛŒØªØ§Ø³Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯!\n",
      "  - Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ: 5000\n",
      "  - Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: 500\n"
     ]
    }
   ],
   "source": [
    "#========================== Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª CNN/DailyMail\n",
    "print(\" Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª \")\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "#========================== Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØªØ§Ø³Øª\n",
    "print(f\" Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØªØ§Ø³Øª:\")\n",
    "print(f\"  - Train: {len(dataset['train'])} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "print(f\"  - Validation: {len(dataset['validation'])} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "print(f\"  - Test: {len(dataset['test'])} Ù†Ù…ÙˆÙ†Ù‡\")\n",
    "\n",
    "#========================== Ù†Ù…Ø§ÛŒØ´ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡\n",
    "print(\"\\n Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ø³Øª:\")\n",
    "print(f\"Article (Ø§ÙˆÙ„ÛŒÙ† 200 Ú©Ø§Ø±Ø§Ú©ØªØ±): {dataset['train'][0]['article'][:200]}...\")\n",
    "print(f\"Highlights: {dataset['train'][0]['highlights']}\")\n",
    "\n",
    "\n",
    "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¯Ø± Ø³Ù„ÙˆÙ„ Ù‚Ø¨Ù„ (ÛŒØ§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶)\n",
    "if 'MAX_TRAIN_SAMPLES' not in globals():\n",
    "    MAX_TRAIN_SAMPLES = 5000\n",
    "if 'MAX_VAL_SAMPLES' not in globals():\n",
    "    MAX_VAL_SAMPLES = 500\n",
    "\n",
    "train_dataset = dataset['train'].select(range(min(MAX_TRAIN_SAMPLES, len(dataset['train']))))\n",
    "val_dataset = dataset['validation'].select(range(min(MAX_VAL_SAMPLES, len(dataset['validation']))))\n",
    "\n",
    "print(f\" Ø¯ÛŒØªØ§Ø³Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯!\")\n",
    "print(f\"  - Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ: {len(train_dataset)}\")\n",
    "print(f\"  - Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3cc8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5ef41edb4f4a4eaff72af5aeca669d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train data:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce73897558b40c9acd4d24ecfbe3516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation data:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù†Ø¯!\n"
     ]
    }
   ],
   "source": [
    "#========================== ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ ÙˆØ±ÙˆØ¯ÛŒ/Ø®Ø±ÙˆØ¬ÛŒ\n",
    "def format_prompt(article, highlights=None):\n",
    "    \"\"\"\n",
    "    ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„\n",
    "    ÙˆØ±ÙˆØ¯ÛŒ: article (Ù…Ù‚Ø§Ù„Ù‡)\n",
    "    Ø®Ø±ÙˆØ¬ÛŒ: highlights (Ø®Ù„Ø§ØµÙ‡)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"### Article:\n",
    "{article}\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "    \n",
    "    if highlights:\n",
    "        prompt += highlights\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "#========================== ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\"\"\"\n",
    "    #========================== ØªØ±Ú©ÛŒØ¨ ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ\n",
    "    full_texts = [format_prompt(article, highlights) for article, highlights in zip(examples['article'], examples['highlights'])]\n",
    "    \n",
    "    #========================== ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù†\n",
    "    model_inputs = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    labels = []\n",
    "    for i, text in enumerate(full_texts):\n",
    "        #========================== Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ÙˆÙ‚Ø¹ÛŒØª Ø´Ø±ÙˆØ¹ summary Ø¯Ø± Ù…ØªÙ† Ø§ØµÙ„ÛŒ\n",
    "        summary_start = text.find(\"### Summary:\")\n",
    "        if summary_start != -1:\n",
    "            #========================== ØªÙˆÚ©Ù†Ø§ÛŒØ² Ú©Ø±Ø¯Ù† ÙÙ‚Ø· Ø¨Ø®Ø´ prompt (Ù‚Ø¨Ù„ Ø§Ø² summary)\n",
    "            prompt_text = text[:summary_start + len(\"### Summary:\\n\")]\n",
    "            prompt_tokens = tokenizer(prompt_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "            prompt_length = len(prompt_tokens['input_ids'][0])\n",
    "            \n",
    "            #========================== Ø§ÛŒØ¬Ø§Ø¯ labels: -100 Ø¨Ø±Ø§ÛŒ promptØŒ Ùˆ input_ids Ø¨Ø±Ø§ÛŒ summary\n",
    "            input_ids = model_inputs['input_ids'][i]\n",
    "            #========================== Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ prompt_length Ø§Ø² input_ids ØªØ¬Ø§ÙˆØ² Ù†Ú©Ù†Ø¯\n",
    "            prompt_length = min(prompt_length, len(input_ids))\n",
    "            label = [-100] * prompt_length\n",
    "            # input_ids Ø¯Ø± Ø­Ø§Ù„Øª batched=True ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø³ØªØŒ Ù†Ù‡ tensor\n",
    "            label.extend(input_ids[prompt_length:])\n",
    "            \n",
    "            #========================== Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù‡Ù…Ø§Ù‡Ù†Ú¯ÛŒ Ø·ÙˆÙ„\n",
    "            if len(label) < len(input_ids):\n",
    "                label.extend([-100] * (len(input_ids) - len(label)))\n",
    "            elif len(label) > len(input_ids):\n",
    "                label = label[:len(input_ids)]\n",
    "            \n",
    "            labels.append(label)\n",
    "        else:\n",
    "            labels.append([-100] * len(model_inputs['input_ids'][i]))\n",
    "    \n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs\n",
    "\n",
    "#========================== Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "print(\" Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ...\")\n",
    "train_tokenized = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train data\"\n",
    ")\n",
    "\n",
    "print(\" Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ...\")\n",
    "val_tokenized = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation data\"\n",
    ")\n",
    "\n",
    "print(\" Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù†Ø¯!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05862b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ø¯Ø± Ø­Ø§Ù„ ØªÙ†Ø¸ÛŒÙ… QLoRA...\n",
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n",
      " QLoRA ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯!\n"
     ]
    }
   ],
   "source": [
    "#========================== ØªÙ†Ø¸ÛŒÙ… QLoRA Ø¨Ø§ PEFT\n",
    "print(\" Ø¯Ø± Ø­Ø§Ù„ ØªÙ†Ø¸ÛŒÙ… QLoRA...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  #========================== rank - Ù‡Ø±Ú†Ù‡ Ø¨ÛŒØ´ØªØ± Ø¨Ø§Ø´Ø¯ØŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù†Ø¯\n",
    "    lora_alpha=32,  #========================== scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ù\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "#========================== Ø§Ø¹Ù…Ø§Ù„ LoRA Ø¨Ù‡ Ù…Ø¯Ù„\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "#========================== Ù†Ù…Ø§ÛŒØ´ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\" QLoRA ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023dcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trainer Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯!\n",
      " Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´...\n"
     ]
    }
   ],
   "source": [
    "#========================== Import Ø§Ø¶Ø§ÙÛŒ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "#========================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù…ÙˆØ²Ø´\n",
    "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² NUM_EPOCHS Ø§Ø² Ø³Ù„ÙˆÙ„ Ù‚Ø¨Ù„ (ÛŒØ§ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶)\n",
    "if 'NUM_EPOCHS' not in globals():\n",
    "    NUM_EPOCHS = 3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "#========================== Data collator\n",
    "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² DataCollatorForSeq2Seq Ø¨Ø±Ø§ÛŒ summarization Ú©Ù‡ Ø¨Ø§ labels Ø§Ø² Ù‚Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ Ø¨Ù‡ØªØ± Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "#========================== Ø§ÛŒØ¬Ø§Ø¯ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\" Trainer Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯!\")\n",
    "print(\" Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´...\")\n",
    "\n",
    "#========================== Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´\n",
    "trainer.train()\n",
    "\n",
    "print(\" Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf0f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================== Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡\n",
    "import os\n",
    "\n",
    "#========================== Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ† Ø´Ø¯Ù‡\n",
    "fine_tuned_model_path = \"./fine_tuned_model\"\n",
    "fine_tuned_model_abs_path = os.path.abspath(fine_tuned_model_path)\n",
    "\n",
    "print(\" Ø¯Ø± Ø­Ø§Ù„ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„...\")\n",
    "print(f\" Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡: {fine_tuned_model_abs_path}\")\n",
    "\n",
    "trainer.save_model(fine_tuned_model_path)\n",
    "tokenizer.save_pretrained(fine_tuned_model_path)\n",
    "\n",
    "print(\" Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!\")\n",
    "print(f\" Ù…Ø¯Ù„ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ† Ø´Ø¯Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡: {fine_tuned_model_abs_path}\")\n",
    "print(f\"   - ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ LoRA adapter Ø¯Ø± Ø§ÛŒÙ† Ù¾ÙˆØ´Ù‡ Ù‡Ø³ØªÙ†Ø¯\")\n",
    "print(f\"   - Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¬Ø¯Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ + adapter Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯\")\n",
    "\n",
    "\n",
    "#========================== ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡\n",
    "def generate_summary(article, max_length=200):\n",
    "    \"\"\"ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ Ø§Ø² ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡\"\"\"\n",
    "    prompt = format_prompt(article)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    #========================== Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙ‚Ø· Ø¨Ø®Ø´ summary\n",
    "    if \"### Summary:\" in generated_text:\n",
    "        summary = generated_text.split(\"### Summary:\")[-1].strip()\n",
    "    else:\n",
    "        summary = generated_text\n",
    "    \n",
    "    return summary\n",
    "\n",
    "#========================== ØªØ³Øª Ø±ÙˆÛŒ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡\n",
    "print(\"\\n Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_samples = val_dataset.select(range(5))\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Ù†Ù…ÙˆÙ†Ù‡ {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    article = sample['article'][:500]  #========================== ÙÙ‚Ø· 500 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´\n",
    "    true_highlights = sample['highlights']\n",
    "    \n",
    "    print(f\"\\n Ù…Ù‚Ø§Ù„Ù‡ (500 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„):\\n{article}...\")\n",
    "    print(f\"\\n Ø®Ù„Ø§ØµÙ‡ ÙˆØ§Ù‚Ø¹ÛŒ:\\n{true_highlights}\")\n",
    "    \n",
    "    generated_summary = generate_summary(sample['article'])\n",
    "    print(f\"\\n Ø®Ù„Ø§ØµÙ‡ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:\\n{generated_summary}\")\n",
    "    print(\"\\n\" + \"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d790d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================== Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± ROUGE\n",
    "print(\" Ø¯Ø± Ø­Ø§Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± ROUGE...\")\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "#========================== Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª\n",
    "test_samples = val_dataset.select(range(min(100, len(val_dataset))))\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "print(f\"Ø¯Ø± Ø­Ø§Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ {len(test_samples)} Ù†Ù…ÙˆÙ†Ù‡...\")\n",
    "\n",
    "for sample in tqdm(test_samples):\n",
    "    true_highlights = sample['highlights']\n",
    "    generated_summary = generate_summary(sample['article'])\n",
    "    \n",
    "    scores = scorer.score(true_highlights, generated_summary)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "#========================== Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†\n",
    "avg_rouge1 = np.mean(rouge1_scores)\n",
    "avg_rouge2 = np.mean(rouge2_scores)\n",
    "avg_rougeL = np.mean(rougeL_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ROUGE:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ROUGE-1 F1: {avg_rouge1:.4f}\")\n",
    "print(f\"ROUGE-2 F1: {avg_rouge2:.4f}\")\n",
    "print(f\"ROUGE-L F1: {avg_rougeL:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#========================== Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ Ù†Ù…Ø±Ø§Øª\n",
    "print(f\"\\n Ø¢Ù…Ø§Ø± Ù†Ù…Ø±Ø§Øª:\")\n",
    "print(f\"ROUGE-1: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={avg_rouge1:.4f}, Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±={np.std(rouge1_scores):.4f}\")\n",
    "print(f\"ROUGE-2: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={avg_rouge2:.4f}, Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±={np.std(rouge2_scores):.4f}\")\n",
    "print(f\"ROUGE-L: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†={avg_rougeL:.4f}, Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±={np.std(rougeL_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================== Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ - Ù†Ù…Ø§ÛŒØ´ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±\n",
    "print(\"ğŸ” Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ - Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù†ØªØ®Ø¨:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#========================== Ø§Ù†ØªØ®Ø§Ø¨ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±\n",
    "import random\n",
    "random.seed(42)\n",
    "selected_indices = random.sample(range(len(test_samples)), min(5, len(test_samples)))\n",
    "\n",
    "detailed_results = []\n",
    "\n",
    "for idx in selected_indices:\n",
    "    sample = test_samples[idx]\n",
    "    article = sample['article']\n",
    "    true_highlights = sample['highlights']\n",
    "    generated_summary = generate_summary(article)\n",
    "    \n",
    "    #========================== Ù…Ø­Ø§Ø³Ø¨Ù‡ ROUGE Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡\n",
    "    scores = scorer.score(true_highlights, generated_summary)\n",
    "    \n",
    "    detailed_results.append({\n",
    "        'article_length': len(article),\n",
    "        'true_summary_length': len(true_highlights),\n",
    "        'generated_summary_length': len(generated_summary),\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure,\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Ù†Ù…ÙˆÙ†Ù‡ {idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n Ù…Ù‚Ø§Ù„Ù‡ (Ø§ÙˆÙ„ÛŒÙ† 300 Ú©Ø§Ø±Ø§Ú©ØªØ±):\\n{article[:300]}...\")\n",
    "    print(f\"\\n Ø®Ù„Ø§ØµÙ‡ ÙˆØ§Ù‚Ø¹ÛŒ ({len(true_highlights)} Ú©Ø§Ø±Ø§Ú©ØªØ±):\\n{true_highlights}\")\n",
    "    print(f\"\\n Ø®Ù„Ø§ØµÙ‡ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ ({len(generated_summary)} Ú©Ø§Ø±Ø§Ú©ØªØ±):\\n{generated_summary}\")\n",
    "    print(f\"\\n Ù†Ù…Ø±Ø§Øª ROUGE:\")\n",
    "    print(f\"  ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
    "    print(f\"  ROUGE-2: {scores['rouge2'].fmeasure:.4f}\")\n",
    "    print(f\"  ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "#========================== Ø§ÛŒØ¬Ø§Ø¯ DataFrame Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„\n",
    "df_results = pd.DataFrame(detailed_results)\n",
    "print(\"\\n Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¯Ù‡:\")\n",
    "print(df_results.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
